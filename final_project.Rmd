---
title: "final_project"
author: "Shiyuan Wang"
date: "4/6/2021"
output: pdf_document
---

```{r, echo = TRUE, message=FALSE,warning = FALSE}
#LIBRARIES
library(quantmod)
library(tidyquant)
library(fs)
library(jsonlite)
library(httr)
library(XML)
library(rvest)
library(ggplot2)
library(dplyr)
library(googleLanguageR)
```

```{r, echo = TRUE, message=FALSE}
#Final project
##Extracting S&P500 data from the packages
stock_list_tbl <- tq_index("SP500") %>%
  select(symbol, company, weight) %>% 
  arrange(desc(weight))
symbol <- stock_list_tbl$symbol[c(1:5)]
###I do not know how to extract data of BRK.B, so I just delete it.
stock_data <- tq_get(symbol, get = "stock.prices",
                     from = "2021-01-01",to= "2021-04-01")
```

```{r, echo = TRUE} 
stock_data <- stock_data %>% 
  select(symbol, date, open, high, low, close)
stock_data
```


```{r, echo = TRUE} 
### Extract New York Times data
get.ny <- function(a){
  NYTIMES_KEY = "LXrkrR8vlr0RnqKwoA7we3dyCngAdEPk"
  
  x <- fromJSON(paste0("http://api.nytimes.com/svc/search/v2/articlesearch.json?q=",a,"&api-key=LXrkrR8vlr0RnqKwoA7we3dyCngAdEPk", sep=""), flatten = TRUE) %>% data.frame()
  
  list(name = c("persons", "organizations", "subject"), value = c("Stock"), rank = 1:3, major = c("N", "N", "N"))
  # Need to use + to string together separate words
  begin_date <- "20210101"
  end_date <- "20210401"
  baseurl <- paste0("http://api.nytimes.com/svc/search/v2/articlesearch.json?q=",a,
                    "&begin_date=",begin_date,"&end_date=",end_date,
                    "&facet_filter=true&api-key=",NYTIMES_KEY, sep="")
  
  initialQuery <- fromJSON(baseurl)
  maxPages <- round((initialQuery$response$meta$hits[1] / 10)-1) 
  pages <- list()
  for(i in 0:3){
    nytSearch <- fromJSON(paste0(baseurl, "&page=", i), flatten = TRUE) %>% data.frame() 
    message("Retrieving page ", i)
    pages[[i+1]] <- nytSearch 
    Sys.sleep(10) 
  }
  allNYTSearch <- rbind_pages(pages)
  return(allNYTSearch)
}
```

```{r echo=}
company.names <- stock_list_tbl$company[c(1:7,9,10)]
ny.data <- data.frame()
ny.data <- rbind(ny.data,get.ny(company.names[1]))
ny.data <- rbind(ny.data,get.ny(company.names[2]))
ny.data <- rbind(ny.data,get.ny(company.names[3]))
ny.data <- rbind(ny.data,get.ny(company.names[4]))
ny.data <- rbind(ny.data,get.ny(company.names[7]))
ny.data$response.docs.abstract
data <- ny.data[-75,]
```

```{r, echo = TRUE, message=FALSE,warning = FALSE} 
install.packages("sentimentr")
library(sentimentr)
sentiment.score <- c()
for (i in 1:nrow(data)){
  sa <- sentiment_by(data$response.docs.abstract[i])
  sentiment.score <- c(sentiment.score, sa$ave_sentiment)
}
sentiment.score


```


```{r, echo = TRUE, message=FALSE,warning = FALSE} 
###Google Api sentiment analysis
if (!"devtools" %in% installed.packages()) {
  install.packages("devtools")
}
devtools::install_github("mkearney/googleapis")
library(googleapis)
gl_auth("My First Project-d3965af63fb5.json")
sentiment.score <- c()
for (i in 1:nrow(data)){
  sa <- analyze_sentiment(data$response.docs.abstract[i])
  sa <- as.data.frame(sa)
  sentiment.score <- c(sentiment.score, mean(sa$score))
}
sentiment.score

data$response.docs.pub_date <- format(as.Date(data$response.docs.pub_date), "%Y-%m-%d")
data <- cbind(data, sentiment.score)
factor.data <- data %>% 
  select(response.docs.pub_date, sentiment.score) %>% 
  group_by(response.docs.pub_date) %>% 
  summarise(mean(sentiment.score)) %>% 
  arrange(response.docs.pub_date)
colnames(factor.data) <- c("date", "sentiment.score")
stock_data$date <- as.character(stock_data$date)
factor.data$date <- as.character(factor.data$date)
final.data <- left_join(stock_data, factor.data)
final.data$sentiment.score[is.na(final.data$sentiment.score)] <- 0
final.data

```

```{r, echo = TRUE}
fit1 <- lm(data = final.data, close~open+high+low+sentiment.score)
summary(fit1)
summary.aov(fit1)

price.change <- final.data$close-final.data$open
final.data <- cbind(final.data, price.change)
fit2 <- lm(data = final.data, price.change~sentiment.score+high+low)
summary(fit2)
summary.aov(fit2)




```


